\def\DevnagVersion{2.13}% Assignment 2 for CMSC723
% Finite State Machines

\documentclass[11pt]{article}
\usepackage{latexsym}
\usepackage{listings}
\usepackage{hyperref}
\usepackage[usenames,dvipsnames]{color}
\usepackage[pdftex]{graphicx}


%@modernhindi

\hypersetup{colorlinks=true,linkcolor=blue}

% \date{}
\begin{document}
\begin{center}
{\Large{\textbf{ Natural Language Processing:  }}}\\
\mbox{}\\
{\Large{Assignment 7: Topic Modeling}}\\
% \mbox{}\\
% (\textsc{Not For Credit})\\
\mbox{}\\
{\large{Jordan Boyd-Graber}}\\
\mbox{}\\
{\large{Out: \textbf{3. November 2014}\\Due: \textbf{14. November 2014}}}\\
\end{center}


% \maketitle
\lstset{stringstyle=\ttfamily,language=Python,showstringspaces=False,tabsize=8,frameround=tttt,
		,keywordstyle=\color{Orange}\bfseries, stringstyle=\ttfamily\color{Green}
		,columns=fullflexible,identifierstyle=\ttfamily
		% , commentstyle=\itshape\color{Red}
}

\section*{Introduction} % (fold)
\label{sec:introduction}
As always, check out the Github repository with the course homework templates:

\url{git://github.com/ezubaric/cl1-hw.git}

The code for this homework is in the \texttt{hw7} directory.

This homework is about unsupervised clustering.  You'll complete an implementation of latent Dirichlet allocation and run it on real data, discovering clusters of documents in Wikipedia.  While there's not much programming you have to do for this assignment (probably less than 10 lines of code), it depends on understanding the rest of the code around it.  Don't leave it until the last minute.

\section{Implementing a Gibbs Sampler (20 points)}

\subsection{Changing topic assignments / counts}

Finish implementing the \texttt{change\_topic} function so that it keeps track of the necessary counts so that you can remember the association of terms to topics, documents to topics, and the specific assignments (i.e., you should update three things).

\subsection{Computing the sampling distribution}

Finish implementing the \texttt{sample\_probs} function so that it returns a dictionary that provides the conditional distribution of a token.  This should be an unnormalized distribution (this will make it easier to debug).

In addition to using the unit tests (as usual), there's also a directory of toy data you can try the code out on.

After you've done these things, turn in your completed \texttt{lda.py} file on Moodle.

\section{Running on Real Data (10 Points)}

In the Github repo, there's a directory called \texttt{wiki}.  Run you topic model on this set of 400 random wikipedia pages and examine the topics.  Upload your results as \texttt{topics.txt}.  You'll run to run it at least for 1000 iterations.

\section{Extra Credit (5 points)}

Run Mallet on the same data and take a note of the comparative runtime.  Upload the mallet file as \texttt{mallet.txt}.

\end{document}
